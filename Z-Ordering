#The first step in the process is to build some fictional test data. 
#There are sophisticated methods to do that like Databricks labs Data Generator but 
#it is much simpler to use SQL functions like RANGE to building something useful for our exercise.
sql_statement = """
WITH CTE AS (
SELECT 
    CAST(ABS(RAND() * 50000000) AS INT) AS id,
    CAST(ABS(RAND() * 40) AS INT) + 60 AS heart_rate 
FROM RANGE(100000000)
)
SELECT id, CONCAT('Person ', id) AS name, heart_rate
FROM CTE
"""
spark.sql(sql_statement).limit(5).display()

#Next, save the fictional data into a delta table.
spark.sql(sql_statement).write.mode("overwrite").saveAsTable("source")

#Tables saved to default database will normally live under dbfs:/user/hive/warehouse so we can inspect the underlying parquet files and delta transaction log.
display(dbutils.fs.ls("dbfs:/user/hive/warehouse/source"))

#There are 5 parquet files and each one of them is around 160MB. Let’s run a SELECT query with a high cardinality filter.
%sql
SELECT * FROM source WHERE id = 1000

#To make the stats for all files easy to visualise, define the below function to collect those metrics. 
#The function is just plumbing code around the JSON data contained in transaction log files.
import pyspark.sql.functions as F
from pyspark.sql.types import StructType, StructField, StringType, LongType
def display_stats_for_commit(commit_path):
    col_values = StructType([ 
        StructField("id",LongType(),True),
        StructField("name",StringType(),True),
        StructField("heart_rate",LongType(),True)
      ])
    schema = StructType([ 
        StructField("numRecords", LongType(),True),
        StructField("minValues", col_values, True),
        StructField("maxValues", col_values, True)
      ])
    (spark.read.json(commit_path)
        .select("add")
        .where("add is not null")
        .selectExpr("add.path", "add.size", "add.stats")
        .withColumn("stats", F.from_json(F.col("stats"), schema))
        .withColumn("numRecords", F.expr("stats.numRecords"))
        .withColumn("min_id", F.expr("stats.minValues.id"))
        .withColumn("max_id", F.expr("stats.maxValues.id"))
        .drop("stats")
        .orderBy("path")
        .display()
    )
    
    #Now run the function against the first transaction which created the source table we have. 
    #You can use %fs magic or dbutils.fs.ls to list all files in transaction log folder.
    display_stats_for_commit("dbfs:/user/hive/warehouse/source/_delta_log/00000000000000000000.json")
    
    #Every parquet file has 12.5M records as expected but each one of them has a range of Ids spanning the whole spectrum of 0 till 50M. So Spark from the files statistics knows that record(s) with Id 1000 can be in any one of the files hence all of them have to be scanned.
    spark.table("source").repartition(8, "id").write.mode("overwrite").saveAsTable("repartitioned_by_id")
display_stats_for_commit("dbfs:/user/hive/warehouse/repartitioned_by_id/_delta_log/00000000000000000000.json")

#Let’s try a different method to re-order data points. Spark DataFrames has a method called repartitionByRange that takes number of partitions plus one or more columns. The function will group data in different partitions based on a disjoint range of those columns. The bounds are calculated using reservoir sampling method behind the scenes for performance reasons.
spark.table("source").repartitionByRange(8, "id").write.mode("overwrite").saveAsTable("range_partitioned_by_id")
display_stats_for_commit("dbfs:/user/hive/warehouse/range_partitioned_by_id/_delta_log/00000000000000000000.json")

#Each parquet file now has data for a disjoint range of Ids. You can see that min_id of a certain file is equal to max_id of the previous file plus 1.
#Now run the same SQL query against the new table.
%sql
CACHE 
SELECT * FROM source WHERE id = 1000


Task 3 - ## Going from a single dimension to multi-dimension
#Range partitioning in the preview section is good for cases where most of your queries involve a single high cardinality column filter although there is also a bit of risk around skew. But the major problem comes when we have more than one column for data co-location. Let’s have a look on the below sample data which is just a cartesian product of all integer values from 0 to 3.
import pyspark.sql.functions as F
df1 = spark.range(4).withColumnRenamed("id", "x")
df2 = spark.range(4).withColumnRenamed("id", "y")
df3 = df1.crossJoin(df2)
df3 = df3.repartitionByRange(2, "x", "y").withColumn("partition", F.spark_partition_id())
df3.orderBy("x", "y").show()
#Range partitioning by x and y will produce partitions that are simply bound to x only (at least for this sample data).

#Linear data placement
By now you should be aware that the problem to be solved is around data placement on disk and which file should be chosen to store a certain record of data. Let’s start with a hypothetical use case.

64 records of data across two dimensions x and y
The records are cartesian product of x and y where both of them are integers ranging from 0 to 7
It is required to store the data in 16 files (4 record per file)
The queries we want to optimise can involve both x and y
Add the following snippet to the current notebook. It’s plain Python not really Spark/Delta related in any way so you can also run it in your preferred Python environment.

import itertools
from typing import List
from pprint import pprint
from dataclasses import dataclass
@dataclass
class DataItem:
    x: int
    y: int
@dataclass
class FileInfo:
    items: List[DataItem]
    minx: int = None
    miny: int = None
    maxx: int = None
    maxy: int = None
    
cross_product = list(itertools.product(range(8), range(8)))
items = [DataItem(d[0], d[1])  for d in cross_product]
pprint(items[:4])


The above is data preparation. DataItem is our record to be stored and FileInfo is similar to the metadata we find in delta transaction log where it holds column-level statistics. The first 4 records should look like this.


Image by author
Next we want to find out what file out of the 16 allowed files should each record go into. Part of that as well is to calculate the statistics of each file. Statistics here means min and max values of x and y. In this stage, linear data placement will be used. Linear here means we would process one dimension at a time in a linear fashion. For example, starting with all x=0 there are 8 records for y ranging for 0 to 7. So the records having x=0 and y = 0 to 3, will go into the first file. Records having x=0 and y = 4 to 7 will go into the second file. Then we move on to x=1 and repeat the process. The next snippet loops over all records and assigns each record to be stored in a certain file. It also populates column-level statistics for each file.
def adjust_file_info(file: FileInfo, item: DataItem):
    if file.minx is None or file.minx > item.x:
        file.minx = item.x
    if file.miny is None or file.miny > item.y:
        file.miny = item.y
    if file.maxx is None or file.maxx < item.x:
        file.maxx = item.x
    if file.maxy is None or file.maxy < item.y:
        file.maxy = item.y
def calculate_data_linear_placement(items: List[DataItem]):
    files = {}
    for index, item in enumerate(items):
        order = index // 4
        if not order in files:
            files[order] = FileInfo([])
        file = files[order]
        file.items.append(item)
        adjust_file_info(file, item)
        
    return files
linear_organised_files = calculate_data_linear_placement(items)
pprint(linear_organised_files)



def find_stats_for_records_matching_x_or_y(files, x, y):
    files_scanned = 0
    false_positives = 0
    for k in files.keys():
        file = files[k]
        if not ((file.minx <= x and file.maxx >= x) or (file.miny <= y and file.maxy >=y)):
            continue
        files_scanned += 1
        for item in file.items:
            if item.x != x and item.y != y:
                false_positives += 1
    print(f"files_scanned : {files_scanned}")
    print(f"false_positives: {false_positives}")
find_stats_for_records_matching_x_or_y(linear_organised_files, 2, 3)



def calculate_data_zorder_placement(items: List[DataItem]):
    files = {}
    for index, item in enumerate(items):
        x, y = item.x, item.y
        xb = f"{x:>03b}"
        yb = f"{y:>03b}"
        interleaved = f"{yb[0]}{xb[0]}{yb[1]}{xb[1]}{yb[2]}{xb[2]}"
        zorder = int(interleaved, 2)
        file_placement = zorder // 4
        if not file_placement in files:
            files[file_placement] = FileInfo([])
        file = files[file_placement]
        file.items.append(item)
        adjust_file_info(file, item)
        
    return files
zorder_organised_files = calculate_data_zorder_placement(items)
pprint(zorder_organised_files)


First, an explicit non-default value will be set for the max file size of OPTIMIZE command. For demo purpose, I will not stick with the default of 1GB but will go for 160MB which is roughly the same file size of the original source table files. We are not really after compaction effect here, just the Z-Order effect.
spark.conf.set("spark.databricks.delta.optimize.maxFileSize", str(1024 * 1024 * 160))

Next, OPTIMIZE command is called doing a Z-Order operation on id column. That can be done in Python but it is easier to use SQL here.
%sql
OPTIMIZE source ZORDER BY (id)
The output of the command will show some metrics about the operation done which involves logically deleting the previous 8 files and adding 8 new files.

last_commit_version = spark.conf.get("spark.databricks.delta.lastCommitVersionInSession")
commit_file = f"{last_commit_version.zfill(20)}.json"
display_stats_for_commit(f"dbfs:/user/hive/warehouse/source/_delta_log/{commit_file}")




