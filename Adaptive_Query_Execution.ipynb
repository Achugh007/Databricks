{"cells":[{"cell_type":"code","source":["input = \"/FileStore/tables/train.csv\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"052f5f38","outputId":"a50b7383-174e-4969-fda7-d5705d29dc3a","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e9c7f416-fd29-4457-8cea-486d81c041d9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## **Adaptive Query Execution**"],"metadata":{"id":"Sh_YhUyOq8AA","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4721b645-ef08-4984-9e11-76fb91b997f9","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["In Apache Spark, adaptive query execution (AQE) is a feature that allows the query optimizer to dynamically adjust the execution plan of a query based on the runtime statistics of the data and the cluster resources. AQE can improve the performance of Spark SQL queries by:\n\n1.   Reordering the physical execution plan based on the data distribution and the statistics.\n2.   Pushing down predicates and projections to the data sources.\n3.   Pruning unnecessary partitions and data sources.\n4.   Enabling/disabling broadcast joins and bucketing.\n5.   Enabling/disabling subquery caching.\n\nAQE is enabled by default in Spark SQL and can be fine-tuned by adjusting configuration settings. However, it's important to note that AQE is a complex feature and may not always produce the best plan for all queries. Therefore, it's also important to monitor the performance of the queries and make adjustments as necessary."],"metadata":{"id":"YlLtqbJKr6TN","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bfe8a5e5-fedc-426c-876d-1418ca881351","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\n# Create a Spark session\nspark = SparkSession.builder.appName(\"AQE Example\").getOrCreate()\n\n# Enable AQE by setting spark.sql.adaptive.enabled to true\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n\n# Read a dataframe from a CSV file\ndf = spark.read.csv(input, header=True, inferSchema=True)\n\ndf1 = df.select(df[\"purchase\"]>10000)\n\ndf1.explain()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7WJJp35JrPhk","outputId":"0954e5fa-fa3c-46d6-dd53-ef14b0e06909","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5277dbe4-f2b1-4d39-93b8-3b490bcf44e4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"== Physical Plan ==\n*(1) Project [(purchase#2261 > 10000) AS (purchase > 10000)#2274]\n+- FileScan csv [Purchase#2261] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/FileStore/tables/train.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Purchase:int>\n\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["== Physical Plan ==\n*(1) Project [(purchase#2261 > 10000) AS (purchase > 10000)#2274]\n+- FileScan csv [Purchase#2261] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/FileStore/tables/train.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Purchase:int>\n\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Register the dataframe as a temporary table\ndf.createOrReplaceTempView(\"data\")\n\n# Execute a query using AQE\nresult = spark.sql(\"SELECT * FROM data WHERE Purchase > 10000\")\n\n# Show the query execution plan\nresult.explain()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ef0d347b-b385-4c52-8c4d-e3ff6f3ee687","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"== Physical Plan ==\n*(1) Filter (isnotnull(Purchase#2261) AND (Purchase#2261 > 10000))\n+- FileScan csv [User_ID#2250,Product_ID#2251,Gender#2252,Age#2253,Occupation#2254,City_Category#2255,Stay_In_Current_City_Years#2256,Marital_Status#2257,Product_Category_1#2258,Product_Category_2#2259,Product_Category_3#2260,Purchase#2261] Batched: false, DataFilters: [isnotnull(Purchase#2261), (Purchase#2261 > 10000)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/FileStore/tables/train.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Purchase), GreaterThan(Purchase,10000)], ReadSchema: struct<User_ID:int,Product_ID:string,Gender:string,Age:string,Occupation:int,City_Category:string...\n\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["== Physical Plan ==\n*(1) Filter (isnotnull(Purchase#2261) AND (Purchase#2261 > 10000))\n+- FileScan csv [User_ID#2250,Product_ID#2251,Gender#2252,Age#2253,Occupation#2254,City_Category#2255,Stay_In_Current_City_Years#2256,Marital_Status#2257,Product_Category_1#2258,Product_Category_2#2259,Product_Category_3#2260,Purchase#2261] Batched: false, DataFilters: [isnotnull(Purchase#2261), (Purchase#2261 > 10000)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/FileStore/tables/train.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Purchase), GreaterThan(Purchase,10000)], ReadSchema: struct<User_ID:int,Product_ID:string,Gender:string,Age:string,Occupation:int,City_Category:string...\n\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["This example shows how to enable AQE, read a CSV file as a dataframe, and then register it as a temporary table. Then, it runs a query on this table using AQE. Finally, it shows the query execution plan using the explain() method.\n\nIt's important to note that AQE does not change the query itself, but it adjusts the execution plan of the query based on the statistics and resources available."],"metadata":{"id":"b0ATdy0oxPeC","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"23382a8d-3b4e-41d8-8927-d42877c181b9","inputWidgets":{},"title":""}}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"application/vnd.databricks.v1+notebook":{"notebookName":"Adaptive_Query_Execution","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":311088494380747}},"nbformat":4,"nbformat_minor":0}
