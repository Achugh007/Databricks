from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder.appName("Bucketing Example").getOrCreate()

# Read a dataframe from a CSV file
df = spark.read.csv(input, header=True, inferSchema=True)

# Bucket the dataframe by column "Purchase" into 100 buckets
df.write.format("parquet").mode("overwrite").bucketBy(100, "User_ID").saveAsTable('bucketed_table')



df1=spark.sql("show databases")
df1.show()


tables = spark.sql("show tables").show()


df2=spark.sql("select * from bucketed_table LIMIT 10")
df2.show()
